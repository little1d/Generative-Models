{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 梯度累积测试 ================\n",
      "不使用 optimizer.zero_grad() - 梯度应持续累积\n",
      "迭代 1: 梯度值 - w=-0.587940, b=-0.587940 | 参数值 - w=-0.247159, b=-0.108295\n",
      "迭代 2: 梯度值 - w=-1.173027, b=-1.173027 | 参数值 - w=-0.241279, b=-0.102416\n",
      "迭代 3: 梯度值 - w=-1.752409, b=-1.752409 | 参数值 - w=-0.229549, b=-0.090685\n",
      "迭代 4: 梯度值 - w=-2.323226, b=-2.323226 | 参数值 - w=-0.212025, b=-0.073161\n",
      "迭代 5: 梯度值 - w=-2.882625, b=-2.882625 | 参数值 - w=-0.188793, b=-0.049929\n",
      "迭代 6: 梯度值 - w=-3.427769, b=-3.427769 | 参数值 - w=-0.159966, b=-0.021103\n",
      "迭代 7: 梯度值 - w=-3.955868, b=-3.955868 | 参数值 - w=-0.125689, b=0.013175\n",
      "迭代 8: 梯度值 - w=-4.464216, b=-4.464216 | 参数值 - w=-0.086130, b=0.052734\n",
      "迭代 9: 梯度值 - w=-4.950247, b=-4.950247 | 参数值 - w=-0.041488, b=0.097376\n",
      "迭代 10: 梯度值 - w=-5.411602, b=-5.411602 | 参数值 - w=0.008015, b=0.146878\n",
      "\n",
      "================ 正常训练测试 ================\n",
      "使用 optimizer.zero_grad() - 每次迭代梯度应相同\n",
      "迭代 1: 梯度值 - w=-0.587940, b=-0.587940 | 参数值 - w=-0.247159, b=-0.108295\n",
      "迭代 2: 梯度值 - w=-0.585088, b=-0.585088 | 参数值 - w=-0.241279, b=-0.102416\n",
      "迭代 3: 梯度值 - w=-0.582244, b=-0.582244 | 参数值 - w=-0.235428, b=-0.096565\n",
      "迭代 4: 梯度值 - w=-0.579409, b=-0.579409 | 参数值 - w=-0.229606, b=-0.090742\n",
      "迭代 5: 梯度值 - w=-0.576583, b=-0.576583 | 参数值 - w=-0.223812, b=-0.084948\n",
      "迭代 6: 梯度值 - w=-0.573765, b=-0.573765 | 参数值 - w=-0.218046, b=-0.079182\n",
      "迭代 7: 梯度值 - w=-0.570956, b=-0.570956 | 参数值 - w=-0.212308, b=-0.073445\n",
      "迭代 8: 梯度值 - w=-0.568157, b=-0.568157 | 参数值 - w=-0.206599, b=-0.067735\n",
      "迭代 9: 梯度值 - w=-0.565367, b=-0.565367 | 参数值 - w=-0.200917, b=-0.062054\n",
      "迭代 10: 梯度值 - w=-0.562586, b=-0.562586 | 参数值 - w=-0.195264, b=-0.056400\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.fc(x))\n",
    "\n",
    "\n",
    "# 使用相同的数据点多次输入\n",
    "input_data = torch.tensor([[1.0]])\n",
    "label_data = torch.tensor([[1.0]])\n",
    "\n",
    "print(\"================ 梯度累积测试 ================\")\n",
    "print(\"不使用 optimizer.zero_grad() - 梯度应持续累积\")\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 记录初始权重和偏置\n",
    "initial_weight = model.fc.weight.item()\n",
    "initial_bias = model.fc.bias.item()\n",
    "\n",
    "# 进行10次迭代，不清零梯度\n",
    "for i in range(10):\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, label_data)\n",
    "\n",
    "    # 反向传播（不重置梯度）\n",
    "    loss.backward()\n",
    "\n",
    "    # 打印当前梯度\n",
    "    print(\n",
    "        f\"迭代 {i+1}: 梯度值 - w={model.fc.weight.grad.item():.6f}, b={model.fc.bias.grad.item():.6f} | \"\n",
    "        f\"参数值 - w={model.fc.weight.item():.6f}, b={model.fc.bias.item():.6f}\"\n",
    "    )\n",
    "\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\n================ 正常训练测试 ================\")\n",
    "print(\"使用 optimizer.zero_grad() - 每次迭代梯度应相同\")\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 重置为初始权重和偏置\n",
    "with torch.no_grad():\n",
    "    model.fc.weight[0, 0] = initial_weight\n",
    "    model.fc.bias[0] = initial_bias\n",
    "\n",
    "# 进行10次迭代，每次清零梯度\n",
    "for i in range(10):\n",
    "    # 重置梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, label_data)\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 打印当前梯度\n",
    "    print(\n",
    "        f\"迭代 {i+1}: 梯度值 - w={model.fc.weight.grad.item():.6f}, b={model.fc.bias.grad.item():.6f} | \"\n",
    "        f\"参数值 - w={model.fc.weight.item():.6f}, b={model.fc.bias.item():.6f}\"\n",
    "    )\n",
    "\n",
    "    # 更新参数\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
